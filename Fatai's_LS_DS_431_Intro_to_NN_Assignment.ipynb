{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fatai's_LS_DS_431_Intro_to_NN_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fataik1/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/Fatai's_LS_DS_431_Intro_to_NN_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dVfaLrjLvxvQ"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Neural Networks\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 1*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wxtoY12mwmih"
      },
      "source": [
        "## Define the Following:\n",
        "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
        "\n",
        "### **Input Layer**: An input layer is where we recieve our input from the dataset. It is also exposed to our data and interacts with it directly. A nueral network  is usually drawn with a visible layer with one neuron per input value or column in the dataset.  \n",
        "\n",
        "### **Hidden Layer**: Hidden layers are not exposed to the input. The simplest network structure is to have a single neuron in the hidden layer that directly outputs the value. \n",
        "\n",
        "\n",
        "### **Output Layer**: The output layer or otherwise known as the final layer is  responsible for outputting a value or vector of values that correspond to the format required. \n",
        "\n",
        "### **Neuron**:  building block for neural networks are neurons. A neuron may have two inputs in which case it requires three weights. \n",
        "\n",
        "### **Weight**: They are often initialized to small random values, such as values in the range 0 to 0.3, although more complex initialization schemes can be used.\n",
        "\n",
        "### **Activation Function**:  mapping of summed weighted input to the output of the neuron. It is called an activation function because it governs the threshold at which the neuron is activated and strength of the output signal.\n",
        "\n",
        "### **Node Map**: a visual diagram of the architecture. It's like a flow chart in that it shows the path from inputs to outputs. They are usually color coded and help us understand at a very high level, some of the differences in architecture between kinds of neural networks.\n",
        "\n",
        "### **Perceptron**: A perceptron is a single neuron model that was a precursor to larger neural networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NXuy9WcWzxa4"
      },
      "source": [
        "## Inputs -> Outputs\n",
        "\n",
        "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PlSwIJMC0A8F"
      },
      "source": [
        "#### Your Answer Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6sWR43PTwhSk"
      },
      "source": [
        "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
        "\n",
        "| x1 | x2 | y |\n",
        "|----|----|---|\n",
        "| 0  | 0  | 1 |\n",
        "| 1  | 0  | 1 |\n",
        "| 0  | 1  | 1 |\n",
        "| 1  | 1  | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF25QzcGOeT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = { 'x1': [0,1,0,1],\n",
        "         'x2': [0,0,1,1],\n",
        "         'y':  [1,1,1,0]\n",
        "       }\n",
        "\n",
        "df = pd.DataFrame.from_dict(data).astype('int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sgh7VFGwnXGH",
        "colab": {}
      },
      "source": [
        "##### Your Code Here #####\n",
        "\n",
        "#First lets define the activation functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    sx = sigmoid(x)\n",
        "    return sx * (1-sx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlzaWkGsOeUQ",
        "colab_type": "code",
        "outputId": "8d29aa54-023a-4ed3-e337-a297b862c180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#now we want to add our inputs\n",
        "one = df['x1'].to_list()\n",
        "two = df['x2'].to_list()\n",
        "inputs = np.array([one, two])\n",
        "inputs "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 1],\n",
              "       [0, 0, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL63Jykii0XP",
        "colab_type": "code",
        "outputId": "b5cd253e-55fb-4a10-81ef-992d320aa2bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#outputs\n",
        "outputs = np.array([df['y'].to_list()])\n",
        "outputs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_tGF56Hi2f7",
        "colab_type": "code",
        "outputId": "19589308-28d8-494f-f5be-d4237581576e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Now we want to grab random weights for each input\n",
        "weights = np.random.random((4,1))\n",
        "weights"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.60091343],\n",
              "       [0.22631564],\n",
              "       [0.17729365],\n",
              "       [0.23131756]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3z1eLkhjNFY",
        "colab_type": "code",
        "outputId": "caddfac8-4083-467c-b1f1-4745a6b2e151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# weighted sum of inputs and weights\n",
        "weighted_sum = np.dot(inputs, weights)\n",
        "weighted_sum"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.45763319],\n",
              "       [0.4086112 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB-vb9juk6hY",
        "colab_type": "code",
        "outputId": "2288282e-1cc5-4015-81dd-03d862a880a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Output the activated value\n",
        "activated_output = sigmoid(weighted_sum)\n",
        "activated_output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.61245255],\n",
              "       [0.60075482]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLsRhhrblcQq",
        "colab_type": "code",
        "outputId": "e5b97499-de85-47ff-b1f2-4131aca68191",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# error take difference of output and true values to calculate error\n",
        "error = outputs - activated_output\n",
        "error"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.38754745,  0.38754745,  0.38754745, -0.61245255],\n",
              "       [ 0.39924518,  0.39924518,  0.39924518, -0.60075482]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puMUW11xljX6",
        "colab_type": "code",
        "outputId": "c19ebbfc-e625-46c2-c0d6-788791529d96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# gradient descent/backpropagation\n",
        "adjustments = error * sigmoid_derivative(weighted_sum)\n",
        "adjustments"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.0919861 ,  0.0919861 ,  0.0919861 , -0.14536832],\n",
              "       [ 0.09575834,  0.09575834,  0.09575834, -0.14409012]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lof98A-gljdl",
        "colab_type": "code",
        "outputId": "7da936af-ab4f-4b8e-dc88-2b71b1ec8a76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "weights = weights + np.dot(inputs.T, adjustments)\n",
        "weights"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.60091343,  0.60091343,  0.60091343,  0.60091343],\n",
              "       [ 0.31830174,  0.31830174,  0.31830174,  0.08094731],\n",
              "       [ 0.27305199,  0.27305199,  0.27305199,  0.03320352],\n",
              "       [ 0.419062  ,  0.419062  ,  0.419062  , -0.05814089]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnBmDgOvljmB",
        "colab_type": "code",
        "outputId": "9922aa0d-430a-48fe-c10e-75b50e741651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# iteration to reduce error\n",
        "for iteration in range(10000):\n",
        "    \n",
        "    # Weighted sum of inputs / weights\n",
        "    weighted_sum = np.dot(inputs, weights)\n",
        "    \n",
        "    # Activate!\n",
        "    activated_output = sigmoid(weighted_sum)\n",
        "    \n",
        "    # Cac error\n",
        "    error = outputs - activated_output\n",
        "    \n",
        "    adjustments = error * sigmoid_derivative(weighted_sum)\n",
        "    \n",
        "    # Update the Weights\n",
        "    weights += np.dot(inputs.T, adjustments)\n",
        "    \n",
        "print(\"Weights after training\")\n",
        "print(weights)\n",
        "\n",
        "print(\"Output after training\")\n",
        "print(activated_output)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights after training\n",
            "[[ 0.60091343  0.60091343  0.60091343  0.60091343]\n",
            " [ 1.88845554  1.88845554  1.88845554 -1.77091904]\n",
            " [ 1.88567948  1.88567948  1.88567948 -1.77341965]\n",
            " [ 3.6018433   3.6018433   3.6018433  -3.71663042]]\n",
            "Output after training\n",
            "[[0.99589014 0.99589014 0.99589014 0.00412113]\n",
            " [0.99587876 0.99587876 0.99587876 0.00411088]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xf7sdqVs0s4x"
      },
      "source": [
        "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
        "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
        "\n",
        "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0P2ExmFOeUk",
        "colab_type": "code",
        "outputId": "7b5bb030-6188-46cf-9c50-83ce0db673e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
        "diabetes.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tmnqxhmnIKH",
        "colab_type": "code",
        "outputId": "f9ea0c4b-5556-4c41-e8a3-8e9046306c1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# plot outcome\n",
        "%matplotlib inline\n",
        "diabetes['Outcome'].value_counts().plot(kind='bar')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f86bee5add8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iIPsYqenSnw",
        "colab_type": "code",
        "outputId": "ba980b10-ed8d-41ba-cd34-274b24bdd967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "diabetes['Outcome'].describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    768.000000\n",
              "mean       0.348958\n",
              "std        0.476951\n",
              "min        0.000000\n",
              "25%        0.000000\n",
              "50%        0.000000\n",
              "75%        1.000000\n",
              "max        1.000000\n",
              "Name: Outcome, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Rvylq3FOeUs",
        "colab_type": "text"
      },
      "source": [
        "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRHJeeczOeUu",
        "colab_type": "code",
        "outputId": "c248379c-52dc-4bf4-d420-3f2574eeda39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
        "\n",
        "feats = list(diabetes)[:-1]\n",
        "\n",
        "# test splits\n",
        "X = diabetes.drop('Outcome', axis=1)\n",
        "y = diabetes['Outcome']\n",
        "\n",
        "# scale the data\n",
        "mmscaler = MinMaxScaler()\n",
        "\n",
        "# fit the data\n",
        "X_scaled = pd.DataFrame(mmscaler.fit_transform(X), columns=feats)\n",
        "X_scaled.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.743719</td>\n",
              "      <td>0.590164</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500745</td>\n",
              "      <td>0.234415</td>\n",
              "      <td>0.483333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.427136</td>\n",
              "      <td>0.540984</td>\n",
              "      <td>0.292929</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.396423</td>\n",
              "      <td>0.116567</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.919598</td>\n",
              "      <td>0.524590</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.347243</td>\n",
              "      <td>0.253629</td>\n",
              "      <td>0.183333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.447236</td>\n",
              "      <td>0.540984</td>\n",
              "      <td>0.232323</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.418778</td>\n",
              "      <td>0.038002</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.688442</td>\n",
              "      <td>0.327869</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.198582</td>\n",
              "      <td>0.642325</td>\n",
              "      <td>0.943638</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies   Glucose  ...  DiabetesPedigreeFunction       Age\n",
              "0     0.352941  0.743719  ...                  0.234415  0.483333\n",
              "1     0.058824  0.427136  ...                  0.116567  0.166667\n",
              "2     0.470588  0.919598  ...                  0.253629  0.183333\n",
              "3     0.058824  0.447236  ...                  0.038002  0.000000\n",
              "4     0.000000  0.688442  ...                  0.943638  0.200000\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aQhnB71n5VZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Update this Class #####\n",
        "\n",
        "class Perceptron:\n",
        "    \n",
        "    def __init__(self, niter = 10):\n",
        "        self.niter = niter\n",
        "    \n",
        "    def __sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    def __sigmoid_derivative(self, x):\n",
        "        sig_x = self.__sigmoid(x)\n",
        "        return sig_x * (1 - sig_x)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit training data\n",
        "        X : Training vectors, X.shape : [#samples, #features]\n",
        "        y : Target values, y.shape : [#samples]\n",
        "        \"\"\"\n",
        "\n",
        "        # Randomly Initialize Weights\n",
        "        self.weights = np.random.random((X.shape[1], 1))\n",
        "        self.inputs = X.values.tolist()\n",
        "        self.outputs = y.values.tolist()\n",
        "\n",
        "\n",
        "        for i in range(self.niter):\n",
        "            # Weighted sum of inputs / weights\n",
        "            self.weighted_sum = np.dot(self.inputs, self.weights)\n",
        "\n",
        "            # Activate!\n",
        "            self.activated_outputs = self.__sigmoid(self.weighted_sum)\n",
        "\n",
        "            # Cac error\n",
        "            self.error = self.outputs - self.activated_outputs\n",
        "            self.adjustments = self.error * self.__sigmoid_derivative(self.weighted_sum)\n",
        "\n",
        "            # Update the Weights\n",
        "            self.weights = self.weights + np.dot(np.array(self.inputs).T, self.adjustments)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def net_input(self, X):\n",
        "        return np.dot(X, self.weights)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return class label after unit step\"\"\"\n",
        "        return np.where(self.net_input(X) >= 0.0, 1, -1)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8n3QDREoS68",
        "colab_type": "code",
        "outputId": "fc13299c-7e06-4bd1-ab07-71be8838c1de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bxjo7AAoZyc",
        "colab_type": "code",
        "outputId": "6f194d23-856c-489d-e929-7075bdb73eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "perc_neur = Perceptron(768)\n",
        "perc_neur.fit(X_scaled, y)\n",
        "\n",
        "y_predict = perc_neur.predict(X_scaled)\n",
        "y_predict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1, -1,  1, -1,  1, -1,  1, -1,  1,  1, -1,  1, -1,  1,  1,  1,  1,\n",
              "        1, -1,  1, -1, -1,  1,  1,  1,  1,  1, -1, -1, -1, -1,  1, -1, -1,\n",
              "       -1, -1, -1,  1,  1,  1, -1, -1, -1,  1, -1,  1, -1, -1,  1, -1, -1,\n",
              "       -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1, -1, -1,  1, -1,  1, -1,\n",
              "       -1, -1,  1, -1,  1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,  1,\n",
              "       -1, -1, -1,  1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,  1,  1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1,  1,  1,  1, -1, -1,  1,  1,  1, -1, -1,\n",
              "       -1,  1, -1, -1, -1,  1,  1, -1, -1,  1,  1,  1,  1,  1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
              "       -1,  1,  1, -1, -1, -1,  1, -1, -1, -1, -1,  1,  1, -1, -1, -1, -1,\n",
              "        1,  1, -1, -1, -1,  1, -1,  1, -1,  1, -1, -1, -1, -1, -1,  1,  1,\n",
              "        1,  1,  1, -1, -1,  1,  1, -1,  1, -1,  1,  1,  1, -1, -1, -1, -1,\n",
              "       -1, -1,  1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1,  1,\n",
              "        1, -1, -1, -1, -1, -1,  1, -1, -1,  1,  1, -1, -1, -1,  1,  1,  1,\n",
              "        1, -1, -1, -1,  1,  1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
              "        1, -1, -1, -1,  1, -1,  1, -1, -1,  1, -1,  1, -1, -1,  1,  1, -1,\n",
              "       -1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1,  1,  1, -1, -1,  1, -1,\n",
              "       -1, -1,  1,  1,  1, -1, -1,  1, -1,  1, -1,  1,  1, -1,  1, -1, -1,\n",
              "        1, -1,  1,  1, -1, -1,  1, -1,  1, -1, -1,  1, -1,  1, -1,  1,  1,\n",
              "        1, -1, -1,  1, -1,  1, -1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,  1,  1,\n",
              "        1, -1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1,  1,  1, -1, -1, -1,\n",
              "       -1,  1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1,  1,  1,  1, -1, -1,\n",
              "        1, -1, -1,  1, -1, -1,  1, -1,  1,  1, -1,  1, -1,  1, -1,  1, -1,\n",
              "        1,  1, -1, -1, -1, -1,  1,  1, -1,  1, -1,  1, -1, -1, -1, -1,  1,\n",
              "        1, -1,  1, -1,  1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,  1, -1,\n",
              "       -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1,  1, -1, -1,  1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1,\n",
              "        1, -1, -1, -1,  1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1, -1, -1,\n",
              "        1, -1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,\n",
              "        1, -1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,\n",
              "       -1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
              "        1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
              "       -1,  1,  1, -1, -1, -1,  1, -1,  1, -1,  1, -1,  1, -1,  1, -1, -1,\n",
              "        1, -1, -1,  1, -1, -1, -1, -1,  1,  1, -1,  1, -1, -1, -1, -1,  1,\n",
              "        1, -1,  1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1,  1, -1, -1, -1, -1,  1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,\n",
              "        1,  1,  1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,  1, -1,  1,  1,\n",
              "        1,  1, -1,  1,  1, -1, -1, -1, -1, -1, -1, -1,  1,  1, -1,  1, -1,\n",
              "       -1,  1, -1,  1, -1, -1, -1, -1, -1,  1, -1,  1, -1,  1, -1,  1,  1,\n",
              "       -1, -1, -1, -1,  1,  1, -1, -1, -1,  1, -1,  1,  1, -1, -1,  1, -1,\n",
              "       -1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
              "        1,  1, -1, -1, -1, -1, -1, -1,  1,  1, -1, -1,  1, -1, -1,  1, -1,\n",
              "        1,  1,  1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1,  1, -1, -1, -1,\n",
              "       -1,  1, -1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5LV6DyqpYdw",
        "colab_type": "code",
        "outputId": "29da6825-7fc8-4e93-d50d-118b19bd8016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy_score(y, y_predict)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3489583333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6QR4oAW1xdyu"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
        "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
        "- Try and implement your own backpropagation algorithm.\n",
        "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
      ]
    }
  ]
}